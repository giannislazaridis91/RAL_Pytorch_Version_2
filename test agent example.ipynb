{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# depending on the classification model use, we might need to import other packages\n",
    "#from sklearn import svm\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "from datasets import DatasetUCI\n",
    "from envs import LalEnvFirstAccuracy\n",
    "\n",
    "from estimator import Estimator\n",
    "from helpers import Minibatch, ReplayBuffer\n",
    "from dqn import DQN\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategies to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_sampling = True\n",
    "uncertainty_sampling = True\n",
    "reinforced_active_learning = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment parameterandom_sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIRNAME_TRANSFER = './agents/1-australian-logreg-8-to-1/'\n",
    "DIRNAME_RESULTS = './AL_results/test-agent-australian.p'\n",
    "test_dataset_names = ['waveform']\n",
    "N_STATE_ESTIMATION = 30\n",
    "SUBSET = -1 # Choose -1 for using all datapoints, 0 for even, 1 for odd.\n",
    "SIZE = -1\n",
    "N_JOBS = 1 # Can set more if we want to parallelise.\n",
    "QUALITY_METHOD = metrics.accuracy_score\n",
    "N_EXPERIMENTS = 1000\n",
    "MAX_BATCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the results from the experiments.\n",
    "results_path = r'./AL_results' \n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models for classifier.\n",
    "\n",
    "- Logistic Regression: LogisticRegression(n_jobs=N_JOBS)\n",
    "- SVM: svm.SVC(probability=True)\n",
    "- RF: RandomForestClassifier(50, oob_score=True, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = DatasetUCI(possible_names=test_dataset_names, n_state_estimation=N_STATE_ESTIMATION, subset=SUBSET, size=SIZE)\n",
    "model = LogisticRegression(n_jobs=N_JOBS)\n",
    "env = LalEnvFirstAccuracy(dataset, model, quality_method=QUALITY_METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare AL methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for random sampling and uncertainty sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if random_sampling:\n",
    "    from Test_AL import policy_random\n",
    "if uncertainty_sampling:\n",
    "    from Test_AL import policy_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load RL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if reinforced_active_learning:\n",
    "    from Test_AL import policy_rl\n",
    "    tf.reset_default_graph()\n",
    "    # Load the DQN agent from DIRNAME_TRANSFER\n",
    "    agent = DQN(experiment_dir=DIRNAME_TRANSFER,  observation_length=N_STATE_ESTIMATION, learning_rate=1e-3, batch_size=32, target_copy_factor=0.01, bias_average=0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiemnts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results will be stored in all_results dictionary\n",
    "all_results = {}\n",
    "all_scores_rand = []\n",
    "all_scores_uncert = []\n",
    "all_scores_rl = []\n",
    "all_durations_rand = []\n",
    "all_durations_uncert = []\n",
    "all_durations_rl = []\n",
    "\n",
    "for experiment in range(N_EXPERIMENTS):\n",
    "\n",
    "    print(\"Round {}.\".format(experiment+1))\n",
    "\n",
    "    if random_sampling:\n",
    "        duration = 6\n",
    "        env_rand = copy.deepcopy(env)\n",
    "        state_rand, next_action_rand, reward_rand = env_rand.reset()\n",
    "        done_rand = False\n",
    "        while not(done_rand):\n",
    "            action_rand, done_rand = policy_random(env_rand.n_actions, max_batch=MAX_BATCH)\n",
    "            if done_rand:\n",
    "                duration =+ len(action_rand)\n",
    "                break\n",
    "            _, _, _, done_rand = env_rand.step(action_rand)\n",
    "            duration+=len(action_rand)\n",
    "        all_scores_rand.append(env_rand.episode_qualities)\n",
    "        all_durations_rand.append(duration)\n",
    "\n",
    "    if uncertainty_sampling:\n",
    "        duration = 6\n",
    "        env_uncert = copy.deepcopy(env)\n",
    "        state_uncert, next_action_uncert, reward_uncert = env_uncert.reset()\n",
    "        done_uncert = False\n",
    "        while not(done_uncert):\n",
    "            action_uncert = policy_uncertainty(next_action_uncert[0,:], env_uncert.n_actions, max_batch=MAX_BATCH)\n",
    "            if done_uncert:\n",
    "                duration =+ len(action_uncert)\n",
    "                break            \n",
    "            next_state_uncert, next_action_uncert, reward_uncert, done_uncert = env_uncert.step(action_uncert)\n",
    "            duration+=len(action_uncert)\n",
    "        all_scores_uncert.append(env_uncert.episode_qualities)\n",
    "        all_durations_uncert.append(duration)\n",
    "        \n",
    "    test_batch = 6\n",
    "    duration = 6\n",
    "    if reinforced_active_learning:\n",
    "        env_rl = copy.deepcopy(env)\n",
    "        state_rl, next_action_rl, reward_rl = env_rl.reset()\n",
    "        done_rl = False\n",
    "        while not(done_rl):\n",
    "            test_batch = env_rl._find_batch_size(test_batch, reward_rl, env_rl.n_actions, max_batch=MAX_BATCH)\n",
    "            if test_batch > env_rl.n_actions:\n",
    "                    duration+=test_batch\n",
    "                    done_rl = True\n",
    "            else:\n",
    "                action_rl = policy_rl(agent, state_rl, next_action_rl, test_batch)        \n",
    "                next_state_rl, next_action_rl, reward_rl, done_rl = env_rl.step(action_rl)\n",
    "                state_rl = next_state_rl\n",
    "                duration+=test_batch\n",
    "        all_scores_rl.append(env_rl.episode_qualities)\n",
    "        all_durations_rl.append(duration)\n",
    "\n",
    "# Record the results.\n",
    "all_results['all_durations_rand'] = all_durations_rand\n",
    "all_results['all_scores_uncert'] = all_scores_uncert\n",
    "all_results['all_scores_rl'] = all_scores_rl\n",
    "all_results['all_scores_rand'] = all_scores_rand\n",
    "all_results['all_durations_uncert'] = all_durations_uncert\n",
    "all_results['all_durations_rl'] = all_durations_rl\n",
    "with open(DIRNAME_RESULTS, 'wb') as file:\n",
    "    pkl.dump(all_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_results = pkl.load(open(DIRNAME_RESULTS, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_scores_rand = all_results['all_scores_rand']\n",
    "all_scores_uncert = all_results['all_scores_uncert']\n",
    "all_scores_reinforced_active_learning = all_results['all_scores_rl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Sampling\n",
    "random_sampling_scores = []\n",
    "for i in range(len(all_scores_rand)):\n",
    "    random_sampling_scores.append(np.array(all_scores_rand[i]).max())\n",
    "    \n",
    "# Uncertainty Sampling\n",
    "uncertainty_sampling_scores = []\n",
    "for i in range(len(all_scores_uncert)):\n",
    "    uncertainty_sampling_scores.append(np.array(all_scores_uncert[i]).max())\n",
    "\n",
    "# Reinforced Active Learning\n",
    "reinforced_active_learning_scores = []\n",
    "for i in range(len(all_scores_reinforced_active_learning)):\n",
    "    reinforced_active_learning_scores.append(np.array(all_scores_reinforced_active_learning[i]).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "xpoints_rand = np.array(range(0,len(random_sampling_scores)))\n",
    "ypoints_rand = np.array(random_sampling_scores)\n",
    "# ypoints_rand = gaussian_filter1d(ypoints_rand, sigma=1)\n",
    "plt.plot(xpoints_rand, ypoints_rand, color='m', label='RANDOM')\n",
    "\n",
    "xpoints_rl = np.array(range(0,len(uncertainty_sampling_scores)))\n",
    "ypoints_rl = np.array(uncertainty_sampling_scores)\n",
    "# ypoints_rl = gaussian_filter1d(ypoints_rl, sigma=1)\n",
    "plt.plot(xpoints_rl, ypoints_rl, color='c', label='UNCERTAINTY')\n",
    "\n",
    "xpoints_uncert = np.array(range(0,len(reinforced_active_learning_scores)))\n",
    "ypoints_uncert = np.array(reinforced_active_learning_scores)\n",
    "# ypoints_uncert = gaussian_filter1d(ypoints_uncert, sigma=1)\n",
    "plt.plot(xpoints_uncert, ypoints_uncert, color='k', label='RAL')\n",
    "\n",
    "plt.title(\"ACC per episode\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"ACC\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"Output images/TESTING, Max ACC per episode.png\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
