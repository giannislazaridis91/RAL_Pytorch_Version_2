{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Depending on the classification model use, we might need to import other packages.\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datasets import DatasetUCI\n",
    "from envs import LalEnvFirstAccuracy\n",
    "from helpers import Minibatch, ReplayBuffer\n",
    "from dqn import DQN\n",
    "from Test_AL import policy_rl\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for dataset and model.\n",
    "\n",
    "- australian: 690\n",
    "- breast_cancer: 263\n",
    "- diabetis: 768\n",
    "- flare_solar: 144\n",
    "- german: 1000\n",
    "- heart: 270\n",
    "- mushrooms: 8124\n",
    "- waveform: 5000\n",
    "- wdbc: 569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_STATE_ESTIMATION = 30\n",
    "SIZE = -1\n",
    "SUBSET = -1 # -1 for using all data points, 0 for even, 1 for odd.\n",
    "N_JOBS = 1 # Can set more if we want to parallelise.\n",
    "# Remove the dataset that will be used for testing.\n",
    "# ['australian', 'breast_cancer', 'diabetis', 'flare_solar', 'german', 'heart', 'mushrooms', 'waveform', 'wdbc']\n",
    "# possible_dataset_names = ['breast_cancer', 'diabetis', 'flare_solar', 'german', 'heart', 'mushrooms', 'waveform', 'wdbc']\n",
    "possible_dataset_names = ['mushrooms']\n",
    "test_dataset_names = ['waveform']\n",
    "# The quality is measured according to a given quality measure \"quality_method\". \n",
    "QUALITY_METHOD = metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd() # Find current directory.\n",
    "\n",
    "# Delete following directories if they exist.\n",
    "shutil.rmtree(cwd+'/__pycache__', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/agents', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/AL_results', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/checkpoints', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/summaries', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/Output images', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a dataset that will contain a sample of datapoint from one the indicated classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = DatasetUCI(possible_dataset_names, n_state_estimation=N_STATE_ESTIMATION, subset=SUBSET, size=SIZE)\n",
    "# If we want to measure test error along with training.\n",
    "dataset_test = DatasetUCI(test_dataset_names, n_state_estimation=N_STATE_ESTIMATION, subset=SUBSET, size=SIZE)\n",
    "# dataset_test: Diabetis datasets consists of 768 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = LalEnvFirstAccuracy(dataset, model, quality_method=QUALITY_METHOD)\n",
    "env_test = LalEnvFirstAccuracy(dataset_test, model, quality_method=QUALITY_METHOD)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for training RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIRNAME = './agents/1-australian-logreg-8-to-1/' # The resulting agent of this experiment will be written in a file.\n",
    "\n",
    "# Adaptive batch size.\n",
    "MAX_BATCH = 20 # Maximum batch size per iteration.\n",
    "\n",
    "# Replay buffer parameters.\n",
    "REPLAY_BUFFER_SIZE = 1e4\n",
    "PRIOROTIZED_REPLAY_EXPONENT = 3\n",
    "\n",
    "# Agent parameters.\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_COPY_FACTOR = 0.01\n",
    "BIAS_INITIALIZATION = 0 # Default 0 # will be set to minus half of average duration during warm start experiments.\n",
    "\n",
    "# Warm start parameters.\n",
    "WARM_START_EPISODES = 150 # Reduce for test.\n",
    "NN_UPDATES_PER_WARM_START = 100\n",
    "\n",
    "# Episode simulation parameters.\n",
    "EPSILON_START = 1\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_STEPS = 1000\n",
    "\n",
    "# Training parameters.\n",
    "TRAINING_EPOCHS = 1000 # Reduce for test.\n",
    "TRAINING_EPISODES_PER_EPOCH = 10 # At each training iteration x episodes are simulated.\n",
    "NN_UPDATES_PER_EPOCHS = 60 # At each training iteration x gradient steps are made.\n",
    "\n",
    "# Validation and test parameters.\n",
    "VALIDATION_ITERATIONS = 500 # Reduce for test.\n",
    "TESTING_ITERATIONS = 500 # Reduce for test.\n",
    "VALIDATION_TESTING_FREQUENCY = 100 # Every x iterations, val and test are performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size=REPLAY_BUFFER_SIZE, prior_exp=PRIOROTIZED_REPLAY_EXPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Warm-start the replay buffer with random episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of episode duration to compute average.\n",
    "episode_durations = []\n",
    "episode_scores = []\n",
    "episode_number = 1\n",
    "\n",
    "for _ in range(WARM_START_EPISODES):\n",
    "    \n",
    "    print(\"Episode {}.\".format(episode_number))\n",
    "    # Reset the environment to start a new episode.\n",
    "    # The classifier_state contains vector representation of state of the environment (depends on the classifier).\n",
    "    # The next_action contains vector representations of all actions available to be taken at the next step.\n",
    "    state, next_action, reward = env.reset()\n",
    "    batch = 6\n",
    "    done = False\n",
    "    episode_duration = 6\n",
    "\n",
    "    # Before we reach a terminal state, make steps.\n",
    "    while not done:\n",
    "\n",
    "        # Choose a random action.\n",
    "        batch = random.randint(1,MAX_BATCH)\n",
    "        if batch > env.n_actions:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Getting numbers from 0 to n_actions.\n",
    "        inputNumbers =range(0,env.n_actions)\n",
    "\n",
    "        # Non-repeating using sample() function.\n",
    "        a = np.array(random.sample(inputNumbers, batch))\n",
    "        action = next_action[:,a]\n",
    "        next_state, next_action, reward, done = env.step(a)\n",
    "        # Store the transition in the replay buffer.\n",
    "        buffer_action = []\n",
    "        for _ in range(MAX_BATCH):\n",
    "            buffer_action.append([0,0,0])\n",
    "        for i in range(len(action.T)):\n",
    "            buffer_action[i]=action.T[0]\n",
    "        replay_buffer.store_transition(state, buffer_action, reward, next_state, next_action, done)\n",
    "        # Get ready for next step.\n",
    "        state = next_state\n",
    "\n",
    "        episode_duration += batch\n",
    "\n",
    "    episode_final_acc = env.return_episode_qualities()\n",
    "    episode_scores.append(episode_final_acc[-1])    \n",
    "    episode_durations.append(episode_duration)\n",
    "    episode_number+=1\n",
    "\n",
    "# Compute the average episode duration of episodes generated during the warm start procedure.\n",
    "av_episode_duration = np.mean(episode_durations)\n",
    "BIAS_INITIALIZATION = -av_episode_duration/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for warm-start episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total budget size per episode.\n",
    "# Total number of episodes: 100.\n",
    "xpoints = np.array(range(0,len(episode_durations)))\n",
    "ypoints = np.array([x/len(dataset.train_data) for x in episode_durations])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints)\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot final achieved accuracy per episode.\n",
    "# Total number of episodes: 100.\n",
    "xpoints = np.array(range(0,len(episode_scores)))\n",
    "ypoints = np.array(episode_scores)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(xpoints, ypoints)\n",
    "plt.title(\"Final achieved accuracy per episode\", loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"ACC\")\n",
    "legend_label = \"Maximum ACC: \" + str(max(episode_scores))[:4]\n",
    "plt.legend([legend_label]) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = DQN(experiment_dir=DIRNAME,\n",
    "            observation_length=N_STATE_ESTIMATION,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            target_copy_factor=TARGET_COPY_FACTOR,\n",
    "            bias_average=BIAS_INITIALIZATION,\n",
    "            max_batch=MAX_BATCH,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do updates of the network based on warm start episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(NN_UPDATES_PER_WARM_START):\n",
    "    \n",
    "    # Sample a batch from the replay buffer proportionally to the probability of sampling.\n",
    "    minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "\n",
    "    # Use batch to train an agent. Keep track of temporal difference errors during training.\n",
    "    td_error = agent.train(minibatch)\n",
    "\n",
    "    # Update probabilities of sampling each datapoint proportionally to the error.\n",
    "    replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run multiple training iterations. Each iteration consists of:\n",
    "- Generating episodes following agent's actions with exploration.\n",
    "- Validation and test episodes for evaluating performance.\n",
    "- Q-network updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_episode_rewards = []\n",
    "i_episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_training = True\n",
    "final_episode_scores_training = []\n",
    "final_episode_durations_training = []\n",
    "final_episode_scores_validation = []\n",
    "final_episode_durations_validation = []\n",
    "final_episode_scores_testing = []\n",
    "final_episode_durations_testing = []\n",
    "validation_and_testing_round = 0\n",
    "\n",
    "for iteration in range(TRAINING_EPOCHS):\n",
    "\n",
    "    print(\"ITERATION {}.\".format(iteration+1))\n",
    "    # GENERATE NEW EPISODES.\n",
    "    # Compute epsilon value according to the schedule.\n",
    "    epsilon = max(EPSILON_END, EPSILON_START-iteration*(EPSILON_START-EPSILON_END)/EPSILON_STEPS)\n",
    "\n",
    "    # Simulate training episodes.\n",
    "    episode_scores_training = []\n",
    "    episode_durations_training = []\n",
    "    episode_duration = 6\n",
    "\n",
    "    for _ in range(TRAINING_EPISODES_PER_EPOCH):\n",
    "\n",
    "        # Reset the environment to start a new episode.\n",
    "        state, next_action, reward = env.reset()\n",
    "        batch = 6\n",
    "        done = False\n",
    "        done = False\n",
    "        \n",
    "        # Run an episode.\n",
    "        while not done:\n",
    "            train_batch = batch\n",
    "            train_batch = env._find_batch_size(train_batch, reward, env.n_actions, MAX_BATCH)\n",
    "            if train_batch > env.n_actions:\n",
    "                done = True\n",
    "                break\n",
    "            episode_duration += train_batch\n",
    "            action = agent.get_action(state, next_action, train_batch)\n",
    "            if np.random.ranf() < epsilon:\n",
    "                batch = random.randint(1,MAX_BATCH)\n",
    "                inputNumbers =range(0,env.n_actions)\n",
    "                action = np.array(random.sample(inputNumbers, batch))\n",
    "\n",
    "            # With epsilon probability, take a random action.\n",
    "            # taken_action is a vector that corresponds to a taken action.\n",
    "            taken_action = next_action[:,action]\n",
    "\n",
    "            # Make another step.\n",
    "            next_state, next_action, reward, done = env.step(action)\n",
    "\n",
    "            # Store a step in replay buffer.\n",
    "            buffer_action = []\n",
    "            for _ in range(MAX_BATCH):\n",
    "                buffer_action.append([0,0,0])\n",
    "            for i in range(len(taken_action.T)):\n",
    "                buffer_action[i]=taken_action.T[0]\n",
    "            replay_buffer.store_transition(state, buffer_action, reward, next_state, next_action, done)\n",
    "            # Change a state of environment.\n",
    "            state = next_state\n",
    "\n",
    "            episode_duration += train_batch\n",
    "\n",
    "        episode_final_acc_training = env.return_episode_qualities()\n",
    "        episode_scores_training.append(episode_final_acc_training[-1])\n",
    "        episode_durations_training.append(episode_duration)\n",
    "\n",
    "    final_episode_scores_training.append(episode_scores_training)\n",
    "    final_episode_durations_training.append(episode_durations_training)\n",
    "\n",
    "    # VALIDATION AND TEST EPISODES.\n",
    "    if iteration%VALIDATION_TESTING_FREQUENCY == 0:\n",
    "        \n",
    "        validation_and_testing_round+=1\n",
    "        print(\"Validation and testing round: \", validation_and_testing_round)\n",
    "\n",
    "        # Validation episodes are run. Use env for it.\n",
    "        episode_scores_validation = []\n",
    "        episode_durations_validation = []\n",
    "\n",
    "        for i in range(VALIDATION_ITERATIONS):\n",
    "            print(\"Validation round:\", i)\n",
    "            episode_duration = 6\n",
    "            validation_batch = 6\n",
    "            done = False\n",
    "            state, next_action, reward = env.reset()\n",
    "            while not(done):\n",
    "                validation_batch = env._find_batch_size(validation_batch, reward, env.n_actions, MAX_BATCH)\n",
    "                if validation_batch > env.n_actions:\n",
    "                    done = True\n",
    "                else:\n",
    "                    action = policy_rl(agent, state, next_action, validation_batch)        \n",
    "                    next_state, next_action, reward, done = env.step(action)\n",
    "                    state = next_state\n",
    "                episode_duration += validation_batch\n",
    "            episode_final_acc_validation = env.return_episode_qualities()\n",
    "            episode_scores_validation.append(episode_final_acc_validation[-1])\n",
    "            episode_durations_validation.append(episode_duration)\n",
    "        \n",
    "        # Test episodes are run. Use env_test for it.\n",
    "        episode_scores_testing = []\n",
    "        episode_durations_testing = []\n",
    "\n",
    "        for i in range(TESTING_ITERATIONS):\n",
    "            print(\"Testing round:\", i)\n",
    "            episode_duration = 6\n",
    "            test_batch = 6\n",
    "            done = False\n",
    "            state, next_action, reward = env_test.reset()\n",
    "            while not(done):\n",
    "                test_batch = env_test._find_batch_size(test_batch, reward, env_test.n_actions, MAX_BATCH)\n",
    "                if test_batch > env_test.n_actions:\n",
    "                    done = True\n",
    "                else:\n",
    "                    action = policy_rl(agent, state, next_action, test_batch)       \n",
    "                    next_state, next_action, reward, done = env_test.step(action)\n",
    "                    state = next_state\n",
    "                episode_duration += test_batch\n",
    "                episode_final_acc_testing = env_test.return_episode_qualities()\n",
    "                episode_scores_testing.append(episode_final_acc_testing[-1])\n",
    "                episode_durations_testing.append(episode_duration)\n",
    "\n",
    "        final_episode_scores_validation.append(episode_scores_validation)\n",
    "        final_episode_durations_validation.append(episode_durations_validation)\n",
    "        final_episode_scores_testing.append(episode_scores_testing)\n",
    "        final_episode_durations_testing.append(episode_durations_testing)\n",
    "\n",
    "    # NEURAL NETWORK UPDATES.\n",
    "    for _ in range(NN_UPDATES_PER_EPOCHS):\n",
    "        minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "        td_error = agent.train(minibatch)\n",
    "        replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total number of validation and testing epochs: 10\n",
    "- Total number of episodes per validation and testing epochs: 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the results from the experiments.\n",
    "results_path = r'./Output images' \n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum accuracy per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum accuracy per training epoch (each epoch consists of 10 episodes)\n",
    "# and their respective budgets.\n",
    "max_score_per_training_epoch = []\n",
    "budget_for_max_score = []\n",
    "for i in range(len(final_episode_scores_training)):\n",
    "    max_score_per_training_epoch.append(max(final_episode_scores_training[i]))\n",
    "    minimum_budget_per_max_ACC = []\n",
    "    for j in range(len(final_episode_scores_training[i])):\n",
    "        if final_episode_scores_training[i][j] == max_score_per_training_epoch[i]:\n",
    "            minimum_budget_per_max_ACC.append(final_episode_durations_training[i][j])\n",
    "    budget_for_max_score.append(np.array(minimum_budget_per_max_ACC).min())\n",
    "\n",
    "# Print maximum ACC.\n",
    "print(\"The maximum accuracy is {}.\".format(max(max_score_per_training_epoch)))\n",
    "\n",
    "# Print smallest budget for the maximum ACC.\n",
    "max_ACC = max(max_score_per_training_epoch)\n",
    "minimum_budget = []\n",
    "for i in range(len(max_score_per_training_epoch)):\n",
    "    if max_score_per_training_epoch[i] == max_ACC:\n",
    "        minimum_budget.append(budget_for_max_score[i])\n",
    "print(\"The budget for the maximum accuracy is {}.\".format(np.array(minimum_budget).min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximum achieved accuracy per epoch and the respective budgets.\n",
    "# Total number of iterations (epochs): 1000.\n",
    "# Total number of episodes per epoch: 10.\n",
    "\n",
    "xpoints = np.array(range(0,len(budget_for_max_score)))\n",
    "ypoints = np.array(budget_for_max_score)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='m')\n",
    "plot_label = \"Budget per max ACC. | Unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Budget\")\n",
    "plt.savefig(\"Output images/TRAINING, Budget.png\")\n",
    "\n",
    "xpoints = np.array(range(0,len(budget_for_max_score)))\n",
    "ypoints = np.array([x/len(dataset.train_data) for x in budget_for_max_score])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='k')\n",
    "plot_label = \"Budget (percentage of the UD) per max ACC. | Unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Budget\")\n",
    "plt.savefig(\"Output images/TRAINING, Budget percentage.png\")\n",
    "\n",
    "xpoints = np.array(range(0,len(max_score_per_training_epoch)))\n",
    "ypoints = np.array(max_score_per_training_epoch)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(xpoints, ypoints, color='c')\n",
    "plt.title(\"Max ACC per epoch\", loc = \"left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Max ACC\")\n",
    "legend_1 = \"Maximum ACC: \" + str(np.array(max_score_per_training_epoch).max())[:4]\n",
    "legend_2 = \", \"\n",
    "legend_3 = \"Budget: \" + str(np.array(minimum_budget).min())[:4]\n",
    "plt.legend([legend_1 + legend_2 + legend_3])\n",
    "plt.savefig(\"Output images/TRAINING, Max ACC.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total budget size per episode for the final epoch.\n",
    "# Total number of episodes: 10.\n",
    "\n",
    "xpoints = np.array(range(0,len(final_episode_durations_training[-1])))\n",
    "ypoints = np.array(final_episode_durations_training[-1])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='m')\n",
    "plot_label = \"Budget | Unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget\")\n",
    "\n",
    "xpoints = np.array(range(0,len(final_episode_durations_training[-1])))\n",
    "ypoints = np.array([x/len(dataset.train_data) for x in final_episode_durations_training[-1]])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='k')\n",
    "plot_label = \"Budget size (percentage of the UD) | Unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget\")\n",
    "\n",
    "xpoints = np.array(range(0,len(final_episode_scores_training[-1])))\n",
    "ypoints = np.array(final_episode_scores_training[-1])\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(xpoints, ypoints, color='c')\n",
    "plt.title(\"ACC per episode | Final epoch\", loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"ACC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for validation episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum accuracy per validation epoch (each epoch consists of 500 episodes).\n",
    "# and their respective budgets.\n",
    "max_score_per_validation_epoch = []\n",
    "budget_for_max_score = []\n",
    "min = 0\n",
    "for i in range(len(final_episode_scores_validation)):\n",
    "    max_score_per_validation_epoch.append(np.array(final_episode_scores_validation[i]).max())\n",
    "    for j in range(len(final_episode_scores_validation[i])):\n",
    "        if final_episode_scores_validation[i][j] == max_score_per_validation_epoch[i]:\n",
    "            min = final_episode_durations_validation[i][j]\n",
    "            for k in range(1, len(final_episode_durations_validation[i])):\n",
    "                if final_episode_durations_validation[i][k] < min:\n",
    "                    min = final_episode_durations_validation[i][k]\n",
    "            budget_for_max_score.append(min)\n",
    "            break\n",
    "print(\"The maximum accuracy is {}.\".format(np.array(max_score_per_validation_epoch).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximum achieved accuracy per validation iteration and the respective budgets.\n",
    "# Total number of iterations (epochs): 10.\n",
    "# Total number of episodes per iteration: 500.\n",
    "\n",
    "xpoints = np.array(range(0,len(budget_for_max_score)))\n",
    "ypoints = np.array(budget_for_max_score)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='m')\n",
    "plot_label = \"Mean budget per max ACC. | Unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Mean budget\")\n",
    "\n",
    "xpoints = np.array(range(0,len(budget_for_max_score)))\n",
    "ypoints = np.array([x/len(dataset.train_data) for x in budget_for_max_score])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='k')\n",
    "plot_label = \"Mean budget (percentage of the UD) per max ACC. | Unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Mean budget\")\n",
    "\n",
    "xpoints = np.array(range(0,len(max_score_per_validation_epoch)))\n",
    "ypoints = np.array(max_score_per_validation_epoch)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(xpoints, ypoints, color='c')\n",
    "plt.title(\"Max ACC per iteration\", loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Max ACC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for testing episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum accuracy per testing epoch (each epoch consists of 500 episodes).\n",
    "# and their respective budgets.\n",
    "max_score_per_testing_epoch = []\n",
    "budget_for_max_score = []\n",
    "min = 0\n",
    "for i in range(len(final_episode_scores_testing)):\n",
    "    max_score_per_testing_epoch.append(np.array(final_episode_scores_testing[i]).max())\n",
    "    minimum_budget_per_max_ACC = []\n",
    "    for j in range(len(final_episode_scores_testing[i])):\n",
    "        if final_episode_scores_testing[i][j] == max_score_per_testing_epoch[i]:\n",
    "            minimum_budget_per_max_ACC.append(final_episode_durations_testing[i][j])\n",
    "    budget_for_max_score.append(np.array(minimum_budget_per_max_ACC).min())\n",
    "\n",
    "# Print maximum ACC.\n",
    "print(\"The maximum accuracy is {}.\".format(max(max_score_per_testing_epoch)))\n",
    "\n",
    "# Print smallest budget for the maximum ACC.\n",
    "max_ACC = max(max_score_per_testing_epoch)\n",
    "minimum_budget = []\n",
    "for i in range(len(max_score_per_testing_epoch)):\n",
    "    if max_score_per_testing_epoch[i] == max_ACC:\n",
    "        minimum_budget.append(budget_for_max_score[i])\n",
    "print(\"The budget for the maximum accuracy is {}.\".format(np.array(minimum_budget).min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximum achieved accuracy per testing iteration and the respective budgets.\n",
    "# Total number of iterations (epochs): 10.\n",
    "# Total number of episodes per iteration: 500.\n",
    "\n",
    "xpoints = np.array(range(0,len(budget_for_max_score)))\n",
    "ypoints = np.array(budget_for_max_score)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='m')\n",
    "plot_label = \"Mean budget per max ACC. | Unlabeled data: \" + str(len(dataset.test_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Mean budget\")\n",
    "\n",
    "xpoints = np.array(range(0,len(budget_for_max_score)))\n",
    "ypoints = np.array([x/len(dataset.test_data) for x in budget_for_max_score])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='k')\n",
    "plot_label = \"Mean budget (percentage of the UD) per max ACC. | Unlabeled data: \" + str(len(dataset.test_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Mean budget\")\n",
    "\n",
    "xpoints = np.array(range(0,len(max_score_per_testing_epoch)))\n",
    "ypoints = np.array(max_score_per_testing_epoch)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(xpoints, ypoints, color='c')\n",
    "plt.title(\"Max ACC\", loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Max ACC\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
