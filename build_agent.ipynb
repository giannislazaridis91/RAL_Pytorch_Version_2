{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from dataset import CIFAR10Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier parameters.\n",
    "CLASSIFIER_NUMBER_OF_CLASSES = 10\n",
    "CLASSIFIER_NUMBER_OF_EPOCHS = 50\n",
    "CLASSIFIER_LEARNING_RATE = 0.01\n",
    "CLASSIFIER_BATCH_SIZE = 64\n",
    "\n",
    "# Parameters for both agents.\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 5e4\n",
    "PRIOROTIZED_REPLAY_EXPONENT = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_COPY_FACTOR = 0.01\n",
    "BIAS_INITIALIZATION = 0\n",
    "\n",
    "# BatchAgent's parameters.\n",
    "\n",
    "DIRNAME = './batch_agent/' # The resulting batch_agent of this experiment will be written in a file.\n",
    "\n",
    "WARM_START_EPISODES_BATCH_AGENT = 5\n",
    "NN_UPDATES_PER_EPOCHS_BATCH_AGENT = 50\n",
    "\n",
    "TRAINING_EPOCHS_BATCH_AGENT = 5\n",
    "TRAINING_EPISODES_PER_EPOCH_BATCH_AGENT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Delete following directories if they exist.\n",
    "for directory in [cwd+'/__pycache__', cwd+'/wandb', cwd+'/batch_agent', cwd+'/libact', cwd+'/AL_results', cwd+'/checkpoints', cwd+'/summaries', cwd+'/data']:\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "ROOT_DIR = './data'\n",
    "STATE_DATA = 5000\n",
    "WARM_START_DATA = 5000\n",
    "AGENT_DATA = 30000\n",
    "TEST_METHODS_DATA = 10000\n",
    "\n",
    "# Transformation for data normalization.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset instance.\n",
    "cifar10_dataset = CIFAR10Dataset(ROOT_DIR, STATE_DATA, WARM_START_DATA, AGENT_DATA, TEST_METHODS_DATA, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(cifar10_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(cifar10_dataset.test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Warm-start data are {}.\".format(len(cifar10_dataset.warm_start_data)))\n",
    "print(\"State data are {}.\".format(len(cifar10_dataset.state_data)))\n",
    "print(\"Agent data are {}.\".format(len(cifar10_dataset.agent_data)))\n",
    "print(\"Test-methods data are {}.\".format(len(cifar10_dataset.test_methods_data)))\n",
    "print(\"Test data are {}.\".format(len(cifar10_dataset.test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model.\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=True)\n",
    "        for param in self.resnet18.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Modify the layers to handle smaller input sizes\n",
    "        self.resnet18.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet18.maxpool = nn.Identity()  # Remove the max pooling layer\n",
    "        \n",
    "        num_ftrs = self.resnet18.fc.in_features\n",
    "        self.resnet18.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, 32, 32)\n",
    "        return self.resnet18(x)\n",
    "\n",
    "# Initialize the model and device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "classifier = CNNClassifier()\n",
    "classifier.to(device)\n",
    "\n",
    "# Define the loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PRECISION = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_envs import LalEnvFirstAccuracy\n",
    "batch_env = LalEnvFirstAccuracy(cifar10_dataset, classifier, epochs=CLASSIFIER_NUMBER_OF_EPOCHS, classifier_batch_size=CLASSIFIER_BATCH_SIZE, target_precision=TARGET_PRECISION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_helpers import ReplayBuffer\n",
    "replay_buffer = ReplayBuffer(buffer_size=REPLAY_BUFFER_SIZE, prior_exp=PRIOROTIZED_REPLAY_EXPONENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear unused memory after each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARM-START EPISODES.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the variables.\n",
    "episode_durations = []\n",
    "episode_scores = []\n",
    "episode_number = 1\n",
    "episode_losses = []\n",
    "episode_precisions = []\n",
    "batches = []\n",
    "\n",
    "# Warm start procedure.\n",
    "for _ in range(WARM_START_EPISODES_BATCH_AGENT):\n",
    "    print(\"Episode {}.\".format(episode_number))\n",
    "    # Reset the environment to start a new episode.\n",
    "    # print(\"- Reset.\")\n",
    "    state, next_action, indicies_unknown, reward = batch_env.reset(code_state=\"Warm-Start\", target_precision=TARGET_PRECISION, target_budget=1.0)\n",
    "    done = False\n",
    "    episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "\n",
    "    # Before we reach a terminal state, make steps.\n",
    "    while not done:\n",
    "        # Choose a random action.\n",
    "        # print(\"-- Number of actions left: {}.\".format(batch_env.n_actions))\n",
    "        batch = torch.randint(1, batch_env.n_actions + 1, (1,)).item()\n",
    "        # print(\"-- Batch: {}.\".format(batch))\n",
    "        batches.append(batch)\n",
    "\n",
    "        # Get the numbers from 0 to n_actions.\n",
    "        input_numbers = range(0, batch_env.n_actions)\n",
    "\n",
    "        # Non-repeating using sample() function.\n",
    "        batch_actions_indices = torch.tensor(np.random.choice(input_numbers, batch, replace=False))\n",
    "        # print(\"batch_actions_indices\", batch_actions_indices)\n",
    "        action = batch\n",
    "        # print(\"- Step.\")\n",
    "        next_state, next_action, indicies_unknown, reward, done = batch_env.step(batch_actions_indices)\n",
    "\n",
    "        if next_action == []:\n",
    "            next_action.append(np.array([0]))\n",
    "\n",
    "        # Store the transition in the replay buffer.\n",
    "        replay_buffer.store_transition(state, action, reward, next_state, next_action, done)\n",
    "\n",
    "        # Get ready for the next step.\n",
    "        state = next_state\n",
    "        episode_duration += batch\n",
    "\n",
    "    # Calculate the final accuracy and precision of the episode.\n",
    "    episode_final_acc = batch_env.return_episode_qualities()     \n",
    "    episode_scores.append(episode_final_acc[-1])\n",
    "    episode_final_precision = batch_env.return_episode_precisions()     \n",
    "    episode_precisions.append(episode_final_precision[-1])    \n",
    "    episode_durations.append(episode_duration)  \n",
    "    episode_number += 1\n",
    "\n",
    "    torch.cuda.empty_cache()  # Clear unused memory after each episode.\n",
    "\n",
    "# Compute the average episode duration of episodes generated during the warm start procedure.\n",
    "av_episode_duration = np.mean(episode_durations)\n",
    "BIAS_INITIALIZATION = - av_episode_duration / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Plot total budget size per episode.\n",
    "xpoints = torch.tensor(range(0, len(episode_durations)))\n",
    "ypoints = torch.tensor(episode_durations)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(xpoints, ypoints, 'o', color='m')  # Plot points as blue circles.\n",
    "xnew = torch.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = interp1d(xpoints, ypoints, kind='cubic')\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='m')\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(cifar10_dataset.warm_start_data))\n",
    "plt.title(plot_label, loc='left')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot total budget size (percentage of the UD) per episode.\n",
    "xpoints = torch.tensor(range(0, len(episode_durations)))\n",
    "ypoints = torch.tensor([x/len(cifar10_dataset.warm_start_data) for x in episode_durations])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(xpoints, ypoints, 'o', color='k')  # Plot points as blue circles.\n",
    "xnew = torch.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = interp1d(xpoints, ypoints, kind='cubic')\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='k')\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(cifar10_dataset.warm_start_data))\n",
    "plt.title(plot_label, loc='left')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot final achieved accuracy per episode.\n",
    "xpoints = torch.tensor(range(0, len(episode_scores)))\n",
    "ypoints = torch.tensor(episode_scores)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(xpoints, ypoints, 'o', color='c')  # Plot points as blue circles.\n",
    "xnew = torch.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = interp1d(xpoints, ypoints, kind='cubic')\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='c')\n",
    "plt.title(\"Final achieved accuracy per episode\", loc='left')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"ACC\")\n",
    "legend_label = \"Maximum ACC: \" + str(max(episode_scores))[:4]\n",
    "plt.legend([legend_label])\n",
    "\n",
    "# Plot final achieved precision per episode.\n",
    "xpoints = torch.tensor(range(0, len(episode_precisions)))\n",
    "ypoints = torch.tensor(episode_precisions)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(xpoints, ypoints, 'o', color='y')  # Plot points as blue circles.\n",
    "xnew = torch.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = interp1d(xpoints, ypoints, kind='cubic')\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='y')\n",
    "plt.title(\"Final achieved precision per episode\", loc='left')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Precision\")\n",
    "legend_label = \"Maximum precision: \" + str(max(episode_precisions))[:4]\n",
    "plt.legend([legend_label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the list to a PyTorch tensor.\n",
    "episode_precisions = torch.tensor(episode_precisions)\n",
    "max_precision = torch.max(episode_precisions)\n",
    "\n",
    "warm_start_batches = []\n",
    "i=0\n",
    "for precision in episode_precisions:\n",
    "    if precision >= max(episode_precisions):\n",
    "        warm_start_batches.append(episode_durations[i])\n",
    "    i+=1\n",
    "TARGET_BUDGET = min(warm_start_batches)/(len(cifar10_dataset.warm_start_data))\n",
    "print(\"Target budget is {}.\".format(TARGET_BUDGET))\n",
    "# TARGET_PRECISION = max(episode_precisions)\n",
    "print(\"Target precision is {}.\".format(TARGET_PRECISION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_dqn import DQN\n",
    "batch_agent = DQN(\n",
    "            observation_length=STATE_DATA,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            target_copy_factor=TARGET_COPY_FACTOR,\n",
    "            bias_average=BIAS_INITIALIZATION,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for update in range(NN_UPDATES_PER_EPOCHS_BATCH_AGENT):\n",
    "    print(\"Update:\", update+1)\n",
    "    minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "    td_error = batch_agent.train(minibatch)\n",
    "    replay_buffer.update_td_errors(td_error, minibatch.indices)\n",
    "    torch.cuda.empty_cache()  # Clear unused memory after each update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH-AGENT TRAINING.\n",
    "\n",
    "# Initialize the agent.\n",
    "agent_epoch_durations = []\n",
    "agent_epoch_scores = []\n",
    "agent_epoch_precisions = []\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS_BATCH_AGENT):\n",
    "    print(\"Training epoch {}.\".format(epoch+1))\n",
    "\n",
    "    # Simulate training episodes.\n",
    "    agent_episode_durations = []\n",
    "    agent_episode_scores = []\n",
    "    agent_episode_precisions = []\n",
    "\n",
    "    for training_episode in range(TRAINING_EPISODES_PER_EPOCH_BATCH_AGENT):\n",
    "\n",
    "        print(\"- Training episode {}.\".format(training_episode+1))\n",
    "\n",
    "        # Reset the environment to start a new episode.\n",
    "        print(\"- Reset.\")\n",
    "        state, action_batch, action_unlabeled_data, reward = batch_env.reset(code_state=\"Agent\", target_precision=TARGET_PRECISION, target_budget=TARGET_BUDGET)\n",
    "        done = False\n",
    "        episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "        first_batch = True\n",
    "\n",
    "        # Run an episode.\n",
    "        while not done:\n",
    "            if first_batch:\n",
    "                next_batch = action_batch\n",
    "                next_unlabeled_data = action_unlabeled_data\n",
    "                first_batch = False\n",
    "            else:\n",
    "                next_batch = next_action_batch_size\n",
    "                next_unlabeled_data = next_action_unlabeled_data\n",
    "\n",
    "            selected_batch, selected_indices = batch_agent.get_action(code_state=\"Agent\", dataset=cifar10_dataset, model=classifier, state=state, next_action_batch=next_batch, next_action_unlabeled_data=next_unlabeled_data)\n",
    "            print(\"- Step.\")\n",
    "            next_state, next_action_batch_size, next_action_unlabeled_data, reward, done = batch_env.step(selected_indices)\n",
    "            if next_action_batch_size==[]:\n",
    "                next_action_batch_size.append(np.array([0]))\n",
    "\n",
    "            print(\"- Buffer.\")\n",
    "            replay_buffer.store_transition(state, selected_batch, reward, next_state, next_action_batch_size, done)\n",
    "        \n",
    "            # Change the state of the environment.\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "            episode_duration += selected_batch\n",
    "            print(\"---   Selected batch is {}.\".format(selected_batch))\n",
    "\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        agent_episode_final_acc = batch_env.return_episode_qualities()\n",
    "        agent_episode_scores.append(agent_episode_final_acc[-1])\n",
    "        agent_episode_final_precision = batch_env.return_episode_precisions()\n",
    "        agent_episode_precisions.append(agent_episode_final_precision[-1])\n",
    "        agent_episode_durations.append(episode_duration)\n",
    "        \n",
    "    maximum_epoch_precision = max(agent_episode_precisions)\n",
    "    minimum_batches_for_the_maximum_epoch_precision = []\n",
    "    accuracy_for_the_maximum_epoch_precision = []\n",
    "    for i in range(len(agent_episode_precisions)):\n",
    "        if agent_episode_precisions[i] == maximum_epoch_precision:\n",
    "            minimum_batches_for_the_maximum_epoch_precision.append(agent_episode_durations[i])\n",
    "            accuracy_for_the_maximum_epoch_precision.append(agent_episode_scores[i])\n",
    "    agent_epoch_precisions.append(maximum_epoch_precision)\n",
    "    agent_epoch_scores.append(accuracy_for_the_maximum_epoch_precision)\n",
    "    agent_epoch_durations.append(min(minimum_batches_for_the_maximum_epoch_precision))\n",
    "\n",
    "    torch.cuda.empty_cache()  # Clear unused memory after each episode.\n",
    "\n",
    "    # NEURAL NETWORK UPDATES.\n",
    "    for update in range(NN_UPDATES_PER_EPOCHS_BATCH_AGENT):\n",
    "        minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "        td_error = batch_agent.train(minibatch)\n",
    "        replay_buffer.update_td_errors(td_error, minibatch.indices)\n",
    "        torch.cuda.empty_cache()  # Clear unused memory after each update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precisions.\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "warm_start_xpoints = np.array(range(0,len(episode_precisions)))\n",
    "warm_start_ypoints = np.array([x*100 for x in episode_precisions])\n",
    "warm_start_xnew = np.linspace(warm_start_xpoints.min(), warm_start_xpoints.max(), 150)\n",
    "warm_start_spl = make_interp_spline(warm_start_xpoints, warm_start_ypoints, k=3)\n",
    "warm_start_power_smooth = warm_start_spl(warm_start_xnew)\n",
    "\n",
    "batch_agent_xpoints = np.array(range(0,len(agent_epoch_precisions)))\n",
    "batch_agent_ypoints = np.array([x*100 for x in agent_epoch_precisions])\n",
    "batch_agent_xnew = np.linspace(batch_agent_xpoints.min(), batch_agent_xpoints.max(), 150)\n",
    "batch_agent_spl = make_interp_spline(batch_agent_xpoints, batch_agent_ypoints, k=3)\n",
    "batch_agent_power_smooth = batch_agent_spl(batch_agent_xnew)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(warm_start_xnew, warm_start_power_smooth, color='y', linewidth=2.5)\n",
    "plt.plot(batch_agent_xnew, batch_agent_power_smooth, color='m', linewidth=2.5)\n",
    "plt.legend([\"Warm-start\", \"Agent\"]) \n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Precision\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot budgets.\n",
    "warm_start_xpoints = np.array(range(0,len(episode_durations)))\n",
    "warm_start_ypoints = np.array([(x/len(CIFAR10Dataset.warm_start_data))*100 for x in episode_durations])\n",
    "warm_start_xnew = np.linspace(warm_start_xpoints.min(), warm_start_xpoints.max(), 150)\n",
    "warm_start_spl = make_interp_spline(warm_start_xpoints, warm_start_ypoints, k=3)\n",
    "warm_start_power_smooth = warm_start_spl(warm_start_xnew)\n",
    "\n",
    "batch_agent_xpoints = np.array(range(0,len(agent_epoch_durations)))\n",
    "batch_agent_ypoints = np.array([(x/len(CIFAR10Dataset.agent_data))*100 for x in agent_epoch_durations])\n",
    "batch_agent_xnew = np.linspace(batch_agent_xpoints.min(), batch_agent_xpoints.max(), 150)\n",
    "batch_agent_spl = make_interp_spline(batch_agent_xpoints, batch_agent_ypoints, k=3)\n",
    "batch_agent_power_smooth = batch_agent_spl(batch_agent_xnew)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(warm_start_xnew, warm_start_power_smooth, color='y', linewidth=2.5)\n",
    "plt.plot(batch_agent_xnew, batch_agent_power_smooth, color='m', linewidth=2.5)\n",
    "plt.legend([\"Warm-start\", \"Agent\"]) \n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Budget\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING_EPISODES = 5\n",
    "CLASSIFIER_NUMBER_OF_CLASSES = 10\n",
    "\n",
    "Reinforced_Active_Learning = True\n",
    "RAL_episode_durations = []\n",
    "RAL_episode_scores = []\n",
    "RAL_episode_precisions = []\n",
    "\n",
    "Random_Sampling = True\n",
    "Random_Sampling_episode_durations = []\n",
    "Random_Sampling_episode_scores = []\n",
    "Random_Sampling_episode_precisions = []\n",
    "\n",
    "for episode in range(TESTING_EPISODES):\n",
    "    state, action_batch, action_unlabeled_data, _ = batch_env.reset(code_state=\"Test methods\", target_precision=TARGET_PRECISION, target_budget=TARGET_BUDGET)\n",
    "    random_sampling_episode_durarion = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "    print(\"Testing episode {}.\".format(episode + 1))\n",
    "\n",
    "    # Reinforced Active Learning\n",
    "    if Reinforced_Active_Learning:\n",
    "        print(\"- Reinforced Active Learning.\")\n",
    "        \n",
    "        RAL_episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "        first_batch = True\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if first_batch:\n",
    "                next_batch = action_batch\n",
    "                next_unlabeled_data = action_unlabeled_data\n",
    "                first_batch = False\n",
    "            else:\n",
    "                next_batch = next_action_batch_size\n",
    "                next_unlabeled_data = next_action_unlabeled_data\n",
    "\n",
    "            selected_batch, selected_indices = batch_agent.get_action(code_state=\"Test methods\", dataset=cifar10_dataset, model=classifier, state=state, next_action_batch=next_batch, next_action_unlabeled_data=next_unlabeled_data)\n",
    "            _, next_action_batch_size, next_action_unlabeled_data, _, done = batch_env.step(selected_indices)\n",
    "\n",
    "            RAL_episode_duration += selected_batch\n",
    "\n",
    "        agent_episode_final_acc = batch_env.return_episode_qualities()\n",
    "        RAL_episode_scores.append(agent_episode_final_acc[-1])\n",
    "        agent_episode_final_precision = batch_env.return_episode_precisions()\n",
    "        RAL_episode_precisions.append(agent_episode_final_precision[-1])\n",
    "        RAL_episode_durations.append(RAL_episode_duration)\n",
    "\n",
    "    # Random Sampling\n",
    "    if Random_Sampling:\n",
    "        print(\"- Random Sampling.\")\n",
    "\n",
    "        Random_Sampling_episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            batch = random.randint(1, batch_env.n_actions)\n",
    "            batch_actions_indices = np.array(random.sample(range(0, batch_env.n_actions), batch))\n",
    "            action = batch\n",
    "            _, next_action, _, _, done = batch_env.step(batch_actions_indices)\n",
    "\n",
    "            Random_Sampling_episode_duration += batch\n",
    "\n",
    "        episode_final_acc = batch_env.return_episode_qualities()\n",
    "        Random_Sampling_episode_scores.append(episode_final_acc[-1])\n",
    "        episode_final_precision = batch_env.return_episode_precisions()\n",
    "        Random_Sampling_episode_precisions.append(episode_final_precision[-1])\n",
    "        Random_Sampling_episode_durations.append(Random_Sampling_episode_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precisions.\n",
    "\n",
    "random_sampling_xpoints = np.array(range(0,len(Random_Sampling_episode_precisions)))\n",
    "random_sampling_ypoints = np.array([x*100 for x in Random_Sampling_episode_precisions])\n",
    "random_sampling_xnew = np.linspace(random_sampling_xpoints.min(), random_sampling_xpoints.max(), 150)\n",
    "random_sampling_spl = make_interp_spline(random_sampling_xpoints, random_sampling_ypoints, k=3)\n",
    "random_sampling_power_smooth = random_sampling_spl(random_sampling_xnew)\n",
    "\n",
    "reinforced_active_learning_xpoints = np.array(range(0,len(RAL_episode_precisions)))\n",
    "reinforced_active_learning_ypoints = np.array([x*100 for x in RAL_episode_precisions])\n",
    "reinforced_active_learning_xnew = np.linspace(reinforced_active_learning_xpoints.min(), reinforced_active_learning_xpoints.max(), 150)\n",
    "reinforced_active_learning_spl = make_interp_spline(reinforced_active_learning_xpoints, reinforced_active_learning_ypoints, k=3)\n",
    "reinforced_active_learning_power_smooth = reinforced_active_learning_spl(reinforced_active_learning_xnew)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(random_sampling_xnew, random_sampling_power_smooth, color='y', linewidth=2.5)\n",
    "plt.plot(reinforced_active_learning_xnew, reinforced_active_learning_power_smooth, color='m', linewidth=2.5)\n",
    "plt.legend([\"RS\", \"RAL\"]) \n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Precision\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot budgets.\n",
    "\n",
    "random_sampling_xpoints = np.array(range(0,len(Random_Sampling_episode_durations)))\n",
    "random_sampling_ypoints = np.array([(x/len(cifar10_dataset.test_methods_data))*100 for x in Random_Sampling_episode_durations])\n",
    "random_sampling_xnew = np.linspace(random_sampling_xpoints.min(), random_sampling_xpoints.max(), 150)\n",
    "random_sampling_spl = make_interp_spline(random_sampling_xpoints, random_sampling_ypoints, k=3)\n",
    "random_sampling_power_smooth = random_sampling_spl(random_sampling_xnew)\n",
    "\n",
    "reinforced_active_learning_xpoints = np.array(range(0,len(RAL_episode_durations)))\n",
    "reinforced_active_learning_ypoints = np.array([(x/len(cifar10_dataset.test_methods_data))*100 for x in RAL_episode_durations])\n",
    "reinforced_active_learning_xnew = np.linspace(reinforced_active_learning_xpoints.min(), reinforced_active_learning_xpoints.max(), 150)\n",
    "reinforced_active_learning_spl = make_interp_spline(reinforced_active_learning_xpoints, reinforced_active_learning_ypoints, k=3)\n",
    "reinforced_active_learning_power_smooth = reinforced_active_learning_spl(reinforced_active_learning_xnew)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(random_sampling_xnew, random_sampling_power_smooth, color='y', linewidth=2.5)\n",
    "plt.plot(reinforced_active_learning_xnew, reinforced_active_learning_power_smooth, color='m', linewidth=2.5)\n",
    "plt.legend([\"RS\", \"RAL\"]) \n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Budget\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
